# ==== SPRING BOOT APPLICATION ====
spring.application.name=tabularasa-bi-q1-stream-processor

# ==== SERVER CONFIGURATION ====
server.port=8083

# ==== DATABASE CONFIGURATION ====
spring.datasource.url=jdbc:postgresql://tabularasa_postgres_db:5432/tabularasadb
spring.datasource.username=tabulauser
spring.datasource.password=tabulapass
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.hikari.maximum-pool-size=10
spring.datasource.hikari.minimum-idle=5
spring.datasource.hikari.idle-timeout=30000
spring.datasource.hikari.pool-name=Q1StreamProcessor
spring.datasource.hikari.max-lifetime=600000
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.leak-detection-threshold=30000
spring.datasource.initialization-mode=always
spring.sql.init.mode=always
spring.sql.init.schema-locations=classpath:schema.sql
spring.sql.init.continue-on-error=true

# ==== KAFKA CONFIGURATION ====
spring.kafka.bootstrap-servers=${app.kafka.bootstrap-servers:kafka:9092}
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.consumer.group-id=q1_stream_consumer_group
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.fetch-max-bytes=5242880
spring.kafka.consumer.max-poll-records=500
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.topic.ad-events=ad-events
app.kafka.group-id=q1-processor-group
app.kafka.topics.ad-events=ad-events

# ==== SPARK CONFIGURATION ====
spark.app.name=TabuRasa-BI-Q1-RealTime
spark.master=local[2]
spark.sql.session.timeZone=UTC
spark.streaming.kafka.maxRatePerPartition=10000
spark.streaming.backpressure.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator=com.tabularasa.bi.q1_realtime_stream_processing.serialization.KryoRegistrator
spark.metrics.conf=classpath:metrics.properties
spark.sql.shuffle.partitions=2
spark.kryo.registrationRequired=false
spark.executor.memory=1g
spark.driver.memory=1g
spark.executor.cores=1
spark.default.parallelism=2
spark.streaming.unpersist=true
spark.streaming.receiver.writeAheadLog.enable=true
spark.streaming.checkpoint-location=/tmp/spark_checkpoints
spark.ui.port=4040

# ==== DATA PATHS ====
# Data path for local file testing 
data.sample.path=q1_realtime_stream_processing/data/sample_ad_events.jsonl

# ==== LOGGING ====
logging.level.root=INFO
logging.level.com.tabularasa.bi=DEBUG
logging.level.org.apache.spark=WARN
logging.level.org.apache.kafka=WARN
logging.level.org.hibernate=WARN
logging.file.name=./logs/application.log

spring.profiles.active=simple

# ==== ACTUATOR ENDPOINTS ====
management.endpoints.web.exposure.include=health,info,metrics,prometheus
management.endpoints.web.exposure.exclude=env,beans
management.endpoint.health.show-details=when_authorized
management.endpoint.metrics.enabled=true
management.metrics.export.prometheus.enabled=true
management.health.db.enabled=true

# Ensure actuator uses main server port
management.server.port=${SERVER_PORT:8083}

# ==== DISTRIBUTED TRACING CONFIGURATION ====
management.tracing.sampling.probability=1.0
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces

# Application-specific properties
app.spark.checkpoint-dir=/tmp/spark_checkpoints
app.kafka.topic=ad-events

spring.main.banner-mode=off
app.kafka.bootstrap-servers=${spring.kafka.bootstrap-servers}

# ==== JPA CONFIGURATION ====
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.properties.hibernate.format_sql=true
spring.jpa.properties.hibernate.jdbc.time_zone=UTC
spring.jpa.open-in-view=false