# ==== SPRING BOOT APPLICATION ====
spring.application.name=tabularasa-bi-q1-stream-processor

# ==== SERVER CONFIGURATION ====
server.port=8083

# ==== DATABASE CONFIGURATION ====
spring.datasource.url=jdbc:postgresql://localhost:5432/tabularasadb
spring.datasource.username=tabulauser
spring.datasource.password=tabulapass
spring.datasource.driver-class-name=org.postgresql.Driver
spring.jpa.hibernate.ddl-auto=update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.show-sql=false
spring.jpa.open-in-view=false

# Connection pool settings
spring.datasource.hikari.maximum-pool-size=10
spring.datasource.hikari.minimum-idle=5
spring.datasource.hikari.idle-timeout=30000

# ==== KAFKA CONFIGURATION ====
spring.kafka.bootstrap-servers=localhost:19092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.consumer.group-id=tabularasa-stream-processor
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.topic.ad-events=ad-events
app.kafka.group-id=q1-processor-group
app.kafka.topics.ad-events=ad-events

# ==== SPARK CONFIGURATION ====
spark.app.name=TabularasaBIStreamProcessor
spark.master=local[*]
spark.sql.session.timeZone=UTC
spark.streaming.kafka.maxRatePerPartition=10000
spark.streaming.backpressure.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator=com.tabularasa.bi.q1_realtime_stream_processing.serialization.KryoRegistrator
spark.metrics.conf=classpath:metrics.properties
spark.sql.shuffle.partitions=200
spark.kryo.registrationRequired=false
spark.executor.memory=1g
spark.driver.memory=1g
spark.streaming.unpersist=true
spark.streaming.receiver.writeAheadLog.enable=true
spark.streaming.checkpoint-location=/tmp/spark_checkpoints

# ==== DATA PATHS ====
# Data path for local file testing 
data.sample.path=q1_realtime_stream_processing/data/sample_ad_events.jsonl

# ==== LOGGING ====
logging.level.root=INFO
logging.level.com.tabularasa.bi=DEBUG
logging.level.org.apache.spark=WARN
logging.level.org.apache.kafka=WARN
logging.file.name=./logs/application.log

spring.profiles.active=simple

# ==== ACTUATOR ENDPOINTS ====
management.endpoints.web.exposure.include=health,info,metrics,prometheus
management.endpoints.web.exposure.exclude=env,beans
management.endpoint.health.show-details=when_authorized
management.endpoint.metrics.enabled=true
management.metrics.export.prometheus.enabled=true

# Ensure actuator uses main server port
management.server.port=${SERVER_PORT:8083}

# ==== DISTRIBUTED TRACING CONFIGURATION ====
management.tracing.sampling.probability=1.0
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces

# Application-specific properties
app.spark.checkpoint-dir=/tmp/spark_checkpoints
app.kafka.topic=ad-events

spring.main.banner-mode=off
app.kafka.bootstrap-servers=${spring.kafka.bootstrap-servers}