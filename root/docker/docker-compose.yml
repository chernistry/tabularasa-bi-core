services:
  # ================== REUSABLE CONFIGURATIONS ==================
  x-airflow-common: &airflow-common
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.1}
    env_file:
      - ./airflow_setup/airflow.env
    volumes:
      - ./airflow_setup/dags:/opt/airflow/dags
      - ./airflow_setup/logs:/opt/airflow/logs
      - ./airflow_setup/plugins:/opt/airflow/plugins
      - ./spark_jars:/opt/airflow/spark_jars
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      tabularasa_postgres_db:
        condition: service_healthy
    networks:
      - docker_bi_network

  # ================== DATABASE SERVICES ==================
  # --► PostgreSQL for application data with production optimizations
  tabularasa_postgres_db:
    image: postgres:16.6-alpine  # Latest stable PostgreSQL
    container_name: tabularasa_postgres_db
    ports:
      - "${DB_PORT:-5432}:5432"
    env_file: .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-tabulauser}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-tabulapass}
      - POSTGRES_DB=${POSTGRES_DB:-tabularasadb}
      # Security and performance optimizations
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256 --data-checksums
      - POSTGRES_HOST_AUTH_METHOD=scram-sha-256
      # Performance tuning for production workloads
      - POSTGRES_SHARED_PRELOAD_LIBRARIES=pg_stat_statements
      - POSTGRES_MAX_CONNECTIONS=${DB_MAX_CONNECTIONS:-200}
      - POSTGRES_SHARED_BUFFERS=${DB_SHARED_BUFFERS:-256MB}
      - POSTGRES_EFFECTIVE_CACHE_SIZE=${DB_EFFECTIVE_CACHE_SIZE:-1GB}
      - POSTGRES_WORK_MEM=${DB_WORK_MEM:-4MB}
      - POSTGRES_MAINTENANCE_WORK_MEM=${DB_MAINTENANCE_WORK_MEM:-64MB}
      - POSTGRES_WAL_BUFFERS=${DB_WAL_BUFFERS:-16MB}
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=${DB_CHECKPOINT_COMPLETION_TARGET:-0.7}
      - POSTGRES_DEFAULT_STATISTICS_TARGET=${DB_DEFAULT_STATISTICS_TARGET:-100}
    volumes:
      - tabularasa_pg_data:/var/lib/postgresql/data
      - ./postgresql/init:/docker-entrypoint-initdb.d:ro
      - ./postgresql/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    command: [
      "postgres",
      "-c", "config_file=/etc/postgresql/postgresql.conf",
      "-c", "logging_collector=on",
      "-c", "log_directory=/var/log/postgresql",
      "-c", "log_filename=postgresql-%Y-%m-%d_%H%M%S.log",
      "-c", "log_rotation_age=1d",
      "-c", "log_rotation_size=100MB",
      "-c", "log_min_duration_statement=1000",
      "-c", "log_line_prefix=%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h ",
      "-c", "log_statement=ddl"
    ]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-tabulauser} -d ${POSTGRES_DB:-tabularasadb} -h localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - docker_bi_network
    deploy:
      resources:
        limits:
          memory: ${DB_MEMORY_LIMIT:-2G}
          cpus: '${DB_CPU_LIMIT:-2.0}'
        reservations:
          memory: ${DB_MEMORY_RESERVATION:-1G}
          cpus: '${DB_CPU_RESERVATION:-1.0}'
    labels:
      - "com.tabularasa.service=database"
      - "com.tabularasa.environment=${ENVIRONMENT:-development}"

  # Kafka configured for production-grade streaming
  kafka:
    image: bitnami/kafka:3.8.1  # Latest stable Kafka
    container_name: kafka
    ports:
      - "${KAFKA_PORT:-9092}:9092"
      - "${KAFKA_JMX_PORT:-9999}:9999"  # JMX for monitoring
    environment:
      # Production listener configuration
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9094,JMX://:9999
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_ADVERTISED_HOST:-localhost}:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,JMX:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # KRaft mode configuration (no Zookeeper)
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9094
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_KRAFT_CLUSTER_ID=tabulaRasaBiKafkaCluster123
      # Production settings
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=${KAFKA_AUTO_CREATE_TOPICS:-false}
      - KAFKA_CFG_NUM_PARTITIONS=${KAFKA_DEFAULT_PARTITIONS:-3}
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=${KAFKA_REPLICATION_FACTOR:-1}
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=${KAFKA_REPLICATION_FACTOR:-1}
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=${KAFKA_REPLICATION_FACTOR:-1}
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=${KAFKA_MIN_ISR:-1}
      - KAFKA_CFG_MIN_INSYNC_REPLICAS=${KAFKA_MIN_ISR:-1}
      # Performance and reliability settings
      - KAFKA_CFG_LOG_RETENTION_HOURS=${KAFKA_LOG_RETENTION_HOURS:-168}  # 7 days
      - KAFKA_CFG_LOG_RETENTION_BYTES=${KAFKA_LOG_RETENTION_BYTES:-1073741824}  # 1GB
      - KAFKA_CFG_LOG_SEGMENT_BYTES=${KAFKA_LOG_SEGMENT_BYTES:-134217728}  # 128MB
      - KAFKA_CFG_LOG_CLEANUP_POLICY=${KAFKA_LOG_CLEANUP_POLICY:-delete}
      - KAFKA_CFG_COMPRESSION_TYPE=${KAFKA_COMPRESSION_TYPE:-snappy}
      - KAFKA_CFG_MESSAGE_MAX_BYTES=${KAFKA_MESSAGE_MAX_BYTES:-1048576}  # 1MB
      - KAFKA_CFG_REPLICA_FETCH_MAX_BYTES=${KAFKA_REPLICA_FETCH_MAX_BYTES:-1048576}
      - KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES=${KAFKA_SOCKET_SEND_BUFFER:-102400}
      - KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES=${KAFKA_SOCKET_RECEIVE_BUFFER:-102400}
      - KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES=${KAFKA_SOCKET_REQUEST_MAX_BYTES:-104857600}
      # JVM settings for production
      - KAFKA_HEAP_OPTS=${KAFKA_HEAP_OPTS:--Xmx1G -Xms1G}
      - KAFKA_JVM_PERFORMANCE_OPTS=${KAFKA_JVM_PERFORMANCE_OPTS:--server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35}
      # Monitoring and metrics
      - KAFKA_CFG_JMX_HOSTNAME_VERIFICATION=false
      - KAFKA_CFG_JMX_OPTS=-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka
    volumes:
      - kafka_data:/bitnami/kafka
      - ./kafka/server.properties:/opt/bitnami/kafka/config/server.properties:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - docker_bi_network
    deploy:
      resources:
        limits:
          memory: ${KAFKA_MEMORY_LIMIT:-2G}
          cpus: '${KAFKA_CPU_LIMIT:-2.0}'
        reservations:
          memory: ${KAFKA_MEMORY_RESERVATION:-1G}
          cpus: '${KAFKA_CPU_RESERVATION:-1.0}'
    labels:
      - "com.tabularasa.service=kafka"
      - "com.tabularasa.environment=${ENVIRONMENT:-development}"

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "8088:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local-kafka
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  # ================== SPARK CLUSTER ==================
  # --► Spark master and worker for distributed processing
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HADOOP_USER_NAME=spark
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_DAEMON_MEMORY=1g
      - SPARK_DAEMON_JAVA_OPTS="-Dhadoop.security.authentication=simple -Djavax.security.auth.useSubjectCredsOnly=false -Djava.security.krb5.conf=/dev/null"
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - spark_jars:/opt/bitnami/spark/jars
      - ../q1_realtime_stream_processing/target/q1_realtime_stream_processing-0.0.1-SNAPSHOT.jar:/opt/spark_apps/q1_realtime_stream_processing-0.0.1-SNAPSHOT.jar
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:8080/ || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - docker_bi_network

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - HADOOP_USER_NAME=spark
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - JAVA_HOME=/opt/bitnami/java
      - SPARK_DAEMON_MEMORY=1g
      - SPARK_DAEMON_JAVA_OPTS="-Dhadoop.security.authentication=simple -Djavax.security.auth.useSubjectCredsOnly=false -Djava.security.krb5.conf=/dev/null"
    volumes:
      - spark_jars:/opt/bitnami/spark/jars
      - ../q1_realtime_stream_processing/target/q1_realtime_stream_processing-0.0.1-SNAPSHOT.jar:/opt/spark_apps/q1_realtime_stream_processing-0.0.1-SNAPSHOT.jar
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:8081/ || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    depends_on:
      spark-master:
        condition: service_started
    networks:
      - docker_bi_network

  # ================== AIRFLOW SERVICES ==================
  # Commented out services preserved but improved for future use

  # ================== MONITORING STACK ==================
  # --► Prometheus, Grafana, Jaeger for observability
  prometheus:
    build:
      context: ./prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    user: "65534:65534" # nobody:nogroup
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  grafana:
    image: grafana/grafana:11.1.0
    container_name: grafana
    ports:
      - "3000:3000"
    user: "472"
    volumes:
      - ./grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    env_file: .env
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:3000/api/health | grep -q '\"database\":\"ok\"'"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - docker_bi_network

  jaeger:
    image: jaegertracing/all-in-one:1.57
    container_name: jaeger
    ports:
      - "16686:16686" # Jaeger UI
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    volumes:
      - jaeger_data:/badger
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:16686"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  alertmanager:
    build:
      context: ./alertmanager
      dockerfile: Dockerfile
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    user: "65534:65534" # nobody:nogroup
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

volumes:
  tabularasa_pg_data:
    driver: local
  kafka_data:
    driver: local
  spark_jars:
    driver: local
  prometheus_data:
    driver: local
  jaeger_data:
    driver: local
  alertmanager_data:
    driver: local
  grafana_data_vol:
    driver: local
  airflow_volume:
    driver: local

networks:
  docker_bi_network:
    driver: bridge
    name: tabularasa_bi_network 