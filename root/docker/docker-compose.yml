services:
  # ================== REUSABLE CONFIGURATIONS ==================
  x-airflow-common: &airflow-common
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.1}
    env_file:
      - ./airflow_setup/airflow.env
    volumes:
      - ./airflow_setup/dags:/opt/airflow/dags
      - ./airflow_setup/logs:/opt/airflow/logs
      - ./airflow_setup/plugins:/opt/airflow/plugins
      - ./spark_jars:/opt/airflow/spark_jars
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      tabularasa_postgres_db:
        condition: service_healthy
    networks:
      - docker_bi_network

  # ================== DATABASE SERVICES ==================
  # --► PostgreSQL for Airflow metadata and app data
  tabularasa_postgres_db:
    image: postgres:14.11-alpine
    container_name: tabularasa_postgres_db
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-tabulauser}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-tabulapass}
      - POSTGRES_DB=${POSTGRES_DB:-tabularasadb}
      # Security best practices
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    volumes:
      - tabularasa_pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-tabulauser} -d ${POSTGRES_DB:-tabularasadb}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - docker_bi_network

  # Kafka configured to use KRaft mode (no Zookeeper)
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    ports:
      - "9092:9092"
      - "19092:19092"
    environment:
      # TODO: Replace ALLOW_PLAINTEXT_LISTENER with proper SSL/TLS setup for production
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@localhost:9094
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19092,CONTROLLER://0.0.0.0:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:19092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false
      # Improved configuration for production readiness
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
      # Producer idempotence for exactly-once semantics
      - KAFKA_CFG_ENABLE_IDEMPOTENCE=true
      # Enable KRaft mode
      - KAFKA_KRAFT_CLUSTER_ID=tabulaRasaBiKafkaCluster123
    volumes:
      - kafka_data:/bitnami/kafka
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 15s
      timeout: 10s
      retries: 10
    networks:
      - docker_bi_network

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "8088:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local-kafka
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  # ================== SPARK CLUSTER ==================
  # --► Spark master and worker for distributed processing
  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    ports:
      - "8081:8080"
      - "7077:7077"
    user: "1001:0"
    environment:
      - SPARK_MODE=master
      # TODO: Enable security features for production
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - HADOOP_USER_NAME=spark
      - HOME=/tmp
      - USER_HOME=/tmp
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # Adaptive Query Execution (AQE) settings
      - SPARK_CONF_spark_sql_adaptive_enabled=true
      - SPARK_CONF_spark_sql_adaptive_coalescePartitions_enabled=true
      - SPARK_CONF_spark_sql_adaptive_fetchShuffleBlocksInBatch=true
      - SPARK_CONF_spark_sql_adaptive_skewJoin_enabled=true
      - SPARK_CONF_spark_sql_shuffle_partitions=200
    volumes:
      - ./spark_apps:/opt/spark_apps
      - ./spark_data:/opt/spark_data
      - ../q1_realtime_stream_processing/src/main/resources/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - docker_bi_network

  spark-worker:
    image: bitnami/spark:3.5.1
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8082:8080"
    user: "1001:0"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - HOME=/tmp
      - USER_HOME=/tmp
      - HADOOP_USER_NAME=spark
      # TODO: Enable security features for production
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark_apps:/opt/spark_apps
      - ./spark_data:/opt/spark_data
      - ../q1_realtime_stream_processing/src/main/resources/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - docker_bi_network

  # ================== AIRFLOW SERVICES ==================
  # Commented out services preserved but improved for future use

  # ================== MONITORING STACK ==================
  # --► Prometheus, Grafana, Jaeger for observability
  prometheus:
    build:
      context: ./prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    user: "65534:65534" # nobody:nogroup
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  grafana:
    image: grafana/grafana:11.1.0
    container_name: grafana
    ports:
      - "3000:3000"
    user: "472"
    volumes:
      - ./grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      # TODO: Remove this line for production and use proper secure password
      - GF_SECURITY_DISABLE_INITIAL_ADMIN_PASSWORD_CHANGE=true
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:3000/api/health | grep -q '\"database\":\"ok\"'"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - docker_bi_network

  jaeger:
    image: jaegertracing/all-in-one:1.57
    container_name: jaeger
    ports:
      - "16686:16686" # Jaeger UI
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    volumes:
      - jaeger_data:/badger
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:16686"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  alertmanager:
    build:
      context: ./alertmanager
      dockerfile: Dockerfile
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    user: "65534:65534" # nobody:nogroup
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

  bi-app:
    image: openjdk:17-jdk-slim
    container_name: bi-app
    depends_on:
      kafka:
        condition: service_healthy
      tabularasa_postgres_db:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      - SPRING_PROFILES_ACTIVE=simple
      - SPRING_DATASOURCE_URL=jdbc:postgresql://tabularasa_postgres_db:5432/tabularasadb
      - SPRING_DATASOURCE_USERNAME=tabulauser
      - SPRING_DATASOURCE_PASSWORD=tabulapass
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - HADOOP_USER_NAME=spark
      - HADOOP_HOME=/opt/hadoop
      - JAVA_OPTS="-Xms512m -Xmx1g -Dhadoop.security.authentication=simple -Dorg.apache.hadoop.security.authentication=simple -Djavax.security.auth.useSubjectCredsOnly=false -Djava.security.auth.login.config=/dev/null -Djava.security.krb5.conf=/dev/null -Dspark.hadoop.hadoop.security.authentication=simple -Dspark.kerberos.keytab=none -Dspark.kerberos.principal=none"
      - MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE=health,info,metrics
      - MANAGEMENT_SECURITY_ENABLED=true
      - MANAGEMENT_SECURITY_ROLES=ACTUATOR
    volumes:
      - ../q1_realtime_stream_processing/target/q1_realtime_stream_processing-0.0.1-SNAPSHOT-exec.jar:/app/app.jar
    command: ["java", "-Dhadoop.security.authentication=simple", "-Dorg.apache.hadoop.security.authentication=simple", "-Dspark.hadoop.hadoop.security.authentication=simple", "-Djavax.security.auth.useSubjectCredsOnly=false", "-Djava.security.auth.login.config=/dev/null", "-Djava.security.krb5.conf=/dev/null", "-Dhadoop.home.dir=/", "-DHADOOP_USER_NAME=spark", "-Dspark.kerberos.keytab=none", "-Dspark.kerberos.principal=none", "-jar", "/app/app.jar"]
    user: "1000:1000" # Use non-root user
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - docker_bi_network

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://tabulauser:tabulapass@tabularasa_postgres_db:5432/tabularasadb?sslmode=disable
    ports:
      - "9187:9187"
    depends_on:
      tabularasa_postgres_db:
        condition: service_healthy
    user: "nobody"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9187/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - docker_bi_network

volumes:
  tabularasa_pg_data:
    driver: local
  kafka_data:
    driver: local
  spark_jars:
    driver: local
  prometheus_data:
    driver: local
  jaeger_data:
    driver: local
  alertmanager_data:
    driver: local
  grafana_data_vol:
    driver: local
  airflow_volume:
    driver: local

networks:
  docker_bi_network:
    driver: bridge
    name: tabularasa_bi_network 